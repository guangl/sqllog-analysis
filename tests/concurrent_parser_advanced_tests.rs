//! concurrent_parser.rs çš„é«˜çº§å•å…ƒæµ‹è¯•
//! ä¸“æ³¨äºæµ‹è¯•æœªè¦†ç›–çš„ä»£ç è·¯å¾„å’Œè¾¹ç•Œæƒ…å†µ

#[cfg(test)]
mod advanced_concurrent_parser_tests {
    use sqllog_analysis::config::SqllogConfig;
    use sqllog_analysis::sqllog::concurrent::ConcurrentParser;

    #[cfg(any(
        feature = "exporter-csv",
        feature = "exporter-json",
        feature = "exporter-sqlite",
        feature = "exporter-duckdb"
    ))]
    use sqllog_analysis::exporter::{ExportStats, SyncExporter};

    use std::fs;
    use std::io::Write;
    use std::path::PathBuf;
    use tempfile::TempDir;

    // åˆ›å»ºæµ‹è¯•ç”¨çš„SQLæ—¥å¿—æ–‡ä»¶
    fn create_test_log_file(
        dir: &TempDir,
        name: &str,
        content: &str,
    ) -> PathBuf {
        let file_path = dir.path().join(name);
        let mut file = fs::File::create(&file_path).unwrap();
        writeln!(file, "{}", content).unwrap();
        file_path
    }

    // åˆ›å»ºå¤æ‚çš„æ—¥å¿—å†…å®¹ï¼ˆåŒ…å«ä¸åŒç±»å‹çš„SQLè¯­å¥ï¼‰
    fn create_complex_log_content(
        record_count: usize,
        base_id: usize,
    ) -> String {
        let mut content = String::new();
        let sql_types = ["SELECT", "INSERT", "UPDATE", "DELETE"];
        let tables = ["users", "orders", "products", "logs"];

        for i in 0..record_count {
            let id = base_id + i;
            let sql_type = sql_types[i % sql_types.len()];
            let table = tables[i % tables.len()];

            let sql_statement = match sql_type {
                "SELECT" => {
                    format!("SELECT * FROM {} WHERE id = {}", table, id)
                }
                "INSERT" => format!(
                    "INSERT INTO {} (id, name) VALUES ({}, 'test_{}')",
                    table, id, id
                ),
                "UPDATE" => format!(
                    "UPDATE {} SET name = 'updated_{}' WHERE id = {}",
                    table, id, id
                ),
                "DELETE" => format!("DELETE FROM {} WHERE id = {}", table, id),
                _ => format!("SELECT count(*) FROM {}", table),
            };

            content.push_str(&format!(
                "2025-09-16 20:02:{:02}.{:03} (EP[{}] sess:0x6da8ccef{:x} thrd:414621{} user:EDM_USER_{} trxid:12215445302{} stmt:0x6da900ef{:x}) {};\n",
                (50 + id) % 60,      // åˆ†é’Ÿ
                500 + id % 500,      // æ¯«ç§’
                id % 10,             // EP
                id,                  // ä¼šè¯ID
                id,                  // çº¿ç¨‹ID
                id % 5,              // ç”¨æˆ·ID
                id,                  // äº‹åŠ¡ID
                id,                  // è¯­å¥ID
                sql_statement        // SQLè¯­å¥
            ));
        }
        content
    }

    // åˆ›å»ºåŒ…å«è¯­æ³•é”™è¯¯çš„æ··åˆå†…å®¹
    fn create_error_mixed_content() -> String {
        r#"2025-09-16 20:02:53.100 (EP[0] sess:0x6da8ccef0 thrd:4146217 user:EDM_BASE trxid:122154453026 stmt:0x6da900ef0) SELECT * FROM valid_table_1;
è¿™ä¸æ˜¯æœ‰æ•ˆçš„æ—¥å¿—è¡Œ - ç¼ºå°‘æ—¶é—´æˆ³å’Œç»“æ„
2025-09-16 20:02:53.200 (EP[1] sess:0x6da8ccef1 thrd:4146218 user:EDM_BASE trxid:122154453027 stmt:0x6da900ef1) INSERT INTO valid_table_2 VALUES(1, 'test');
å¦ä¸€ä¸ªé”™è¯¯è¡Œï¼šæ—¶é—´æˆ³æ ¼å¼ä¸æ­£ç¡® 25-09-16 20:02:53.300
2025-09-16 20:02:53.400 (EP[2] sess:0x6da8ccef2 thrd:4146219 user:EDM_BASE trxid:122154453028 stmt:0x6da900ef2) UPDATE valid_table_3 SET col1 = 'value';
é”™è¯¯ï¼šç¼ºå°‘EPä¿¡æ¯ 2025-09-16 20:02:53.500 sess:0x6da8ccef3 thrd:4146220 user:EDM_BASE trxid:122154453029 stmt:0x6da900ef3) DELETE FROM table;
2025-09-16 20:02:53.600 (EP[3] sess:0x6da8ccef4 thrd:4146221 user:EDM_BASE trxid:122154453030 stmt:0x6da900ef4) SELECT COUNT(*) FROM valid_table_4;
å®Œå…¨é”™è¯¯çš„æ ¼å¼ - æ²¡æœ‰ä»»ä½•ç»“æ„
"#.to_string()
    }

    #[test]
    fn test_concurrent_parser_new_with_none_thread_count() {
        let config = SqllogConfig {
            thread_count: None, // None åº”è¯¥è½¬æ¢ä¸º 0
            batch_size: 100,
            queue_buffer_size: 1000,
        };

        let parser = ConcurrentParser::new(config);
        assert_eq!(parser.thread_count, 0); // None -> 0
        assert_eq!(parser.batch_size, 100);
    }

    #[test]
    fn test_concurrent_parser_new_with_some_thread_count() {
        let config = SqllogConfig {
            thread_count: Some(8),
            batch_size: 250,
            queue_buffer_size: 2000,
        };

        let parser = ConcurrentParser::new(config);
        assert_eq!(parser.thread_count, 8);
        assert_eq!(parser.batch_size, 250);
    }

    #[test]
    fn test_concurrent_parser_new_extreme_config_values() {
        // æµ‹è¯•æç«¯é…ç½®å€¼
        let config = SqllogConfig {
            thread_count: Some(0), // æœ€å°å€¼
            batch_size: 1,         // æœ€å°æ‰¹æ¬¡
            queue_buffer_size: 1,  // æœ€å°é˜Ÿåˆ—
        };

        let parser = ConcurrentParser::new(config);
        assert_eq!(parser.thread_count, 0);
        assert_eq!(parser.batch_size, 1);

        let config2 = SqllogConfig {
            thread_count: Some(1000),  // å¾ˆå¤§çš„å€¼
            batch_size: 10000,         // å¾ˆå¤§çš„æ‰¹æ¬¡
            queue_buffer_size: 100000, // å¾ˆå¤§çš„é˜Ÿåˆ—
        };

        let parser2 = ConcurrentParser::new(config2);
        assert_eq!(parser2.thread_count, 1000);
        assert_eq!(parser2.batch_size, 10000);
    }

    #[test]
    fn test_concurrent_parser_default_values() {
        let parser = ConcurrentParser::default();
        let default_config = SqllogConfig::default();

        assert_eq!(
            parser.thread_count,
            default_config.thread_count.unwrap_or(0)
        );
        assert_eq!(parser.batch_size, default_config.batch_size);
    }

    #[test]
    fn test_concurrent_parser_clone() {
        let config = SqllogConfig {
            thread_count: Some(4),
            batch_size: 150,
            queue_buffer_size: 3000,
        };

        let parser1 = ConcurrentParser::new(config);
        let parser2 = parser1.clone();

        assert_eq!(parser1.thread_count, parser2.thread_count);
        assert_eq!(parser1.batch_size, parser2.batch_size);
    }

    #[test]
    fn test_concurrent_parser_debug_format() {
        let parser = ConcurrentParser::new(SqllogConfig {
            thread_count: Some(2),
            batch_size: 50,
            queue_buffer_size: 1000,
        });

        let debug_str = format!("{:?}", parser);
        assert!(debug_str.contains("ConcurrentParser"));
        assert!(debug_str.contains("thread_count"));
        assert!(debug_str.contains("batch_size"));
    }

    #[test]
    fn test_parse_files_concurrent_complex_content() {
        let temp_dir = TempDir::new().unwrap();

        // åˆ›å»ºåŒ…å«å¤æ‚SQLè¯­å¥çš„æ–‡ä»¶
        let complex_content = create_complex_log_content(20, 0);
        let complex_file =
            create_test_log_file(&temp_dir, "complex.log", &complex_content);

        let parser = ConcurrentParser::new(SqllogConfig {
            thread_count: Some(2),
            batch_size: 5,
            queue_buffer_size: 1000,
        });

        let result = parser.parse_files_concurrent(&[complex_file]);
        assert!(result.is_ok());

        let (records, errors) = result.unwrap();
        assert_eq!(records.len(), 20);
        assert_eq!(errors.len(), 0);

        // éªŒè¯SQLè¯­å¥åŒ…å«ä¸åŒçš„æ“ä½œç±»å‹
        let descriptions: Vec<String> =
            records.iter().map(|r| r.description.clone()).collect();

        // æ£€æŸ¥æ˜¯å¦åŒ…å«ä¸åŒç±»å‹çš„SQLæ“ä½œ
        let has_select = descriptions.iter().any(|d| d.contains("SELECT"));
        let has_insert = descriptions.iter().any(|d| d.contains("INSERT"));
        let has_update = descriptions.iter().any(|d| d.contains("UPDATE"));
        let has_delete = descriptions.iter().any(|d| d.contains("DELETE"));

        // è‡³å°‘åº”è¯¥æœ‰2ç§ä¸åŒç±»å‹çš„æ“ä½œ
        let type_count = [has_select, has_insert, has_update, has_delete]
            .iter()
            .filter(|&&x| x)
            .count();
        assert!(type_count >= 2, "åº”è¯¥è§£æå‡ºè‡³å°‘2ç§ä¸åŒç±»å‹çš„SQLæ“ä½œ");
    }

    #[test]
    fn test_parse_files_concurrent_mixed_valid_invalid() {
        let temp_dir = TempDir::new().unwrap();

        let mixed_content = create_error_mixed_content();
        let mixed_file =
            create_test_log_file(&temp_dir, "mixed.log", &mixed_content);

        let parser = ConcurrentParser::new(SqllogConfig {
            thread_count: Some(1),
            batch_size: 3,
            queue_buffer_size: 1000,
        });

        let result = parser.parse_files_concurrent(&[mixed_file]);
        assert!(result.is_ok());

        let (records, _errors) = result.unwrap();
        // åº”è¯¥è§£æå‡ºæœ‰æ•ˆçš„è®°å½•ï¼ˆå¤§çº¦3-4æ¡æœ‰æ•ˆè®°å½•ï¼‰
        assert!(records.len() >= 3);
        assert!(records.len() <= 4);

        // éªŒè¯è§£æå‡ºçš„è®°å½•éƒ½åŒ…å«æœ‰æ•ˆçš„SQLè¯­å¥
        for record in &records {
            assert!(!record.description.is_empty());
            assert!(
                record.description.contains("SELECT")
                    || record.description.contains("INSERT")
                    || record.description.contains("UPDATE")
            );
        }
    }

    #[test]
    fn test_parse_files_concurrent_zero_batch_size() {
        let temp_dir = TempDir::new().unwrap();
        let log_content = create_complex_log_content(10, 0);
        let log_file =
            create_test_log_file(&temp_dir, "zero_batch.log", &log_content);

        let parser = ConcurrentParser::new(SqllogConfig {
            thread_count: Some(1),
            batch_size: 0, // é›¶æ‰¹æ¬¡å¤§å°
            queue_buffer_size: 1000,
        });

        let result = parser.parse_files_concurrent(&[log_file]);
        assert!(result.is_ok());

        let (records, errors) = result.unwrap();
        assert_eq!(records.len(), 10);
        assert_eq!(errors.len(), 0);
    }

    #[test]
    fn test_parse_files_concurrent_file_path_variations() {
        let temp_dir = TempDir::new().unwrap();

        // åˆ›å»ºä¸åŒè·¯å¾„æ·±åº¦çš„æ–‡ä»¶
        let subdir = temp_dir.path().join("subdir").join("deep");
        fs::create_dir_all(&subdir).unwrap();

        let content1 = create_complex_log_content(5, 0);
        let content2 = create_complex_log_content(3, 100);

        let file1 = create_test_log_file(&temp_dir, "root.log", &content1);
        let file2_path = subdir.join("deep.log");
        let mut file2 = fs::File::create(&file2_path).unwrap();
        writeln!(file2, "{}", content2).unwrap();

        let parser = ConcurrentParser::new(SqllogConfig {
            thread_count: Some(2),
            batch_size: 4,
            queue_buffer_size: 1000,
        });

        let result = parser.parse_files_concurrent(&[file1, file2_path]);
        assert!(result.is_ok());

        let (records, errors) = result.unwrap();
        assert_eq!(records.len(), 8); // 5 + 3
        assert_eq!(errors.len(), 0);
    }

    #[test]
    fn test_parse_files_concurrent_unicode_content() {
        let temp_dir = TempDir::new().unwrap();

        // åˆ›å»ºåŒ…å«Unicodeå­—ç¬¦çš„æ—¥å¿—
        let unicode_content = format!(
            "2025-09-16 20:02:53.562 (EP[0] sess:0x6da8ccef0 thrd:4146217 user:ç”¨æˆ·_æµ‹è¯• trxid:122154453026 stmt:0x6da900ef0) SELECT * FROM è¡¨å_æµ‹è¯• WHERE å­—æ®µ = 'ä¸­æ–‡å€¼';\n\
             2025-09-16 20:02:53.563 (EP[1] sess:0x6da8ccef1 thrd:4146218 user:EDM_BASE trxid:122154453027 stmt:0x6da900ef1) INSERT INTO products (name) VALUES ('äº§å“åç§°_ğŸš€');\n\
             2025-09-16 20:02:53.564 (EP[2] sess:0x6da8ccef2 thrd:4146219 user:EDM_BASE trxid:122154453028 stmt:0x6da900ef2) UPDATE orders SET status = 'completed_å®Œæˆ' WHERE id = 123;\n"
        );

        let unicode_file =
            create_test_log_file(&temp_dir, "unicode.log", &unicode_content);

        let parser = ConcurrentParser::new(SqllogConfig {
            thread_count: Some(1),
            batch_size: 2,
            queue_buffer_size: 1000,
        });

        let result = parser.parse_files_concurrent(&[unicode_file]);
        assert!(result.is_ok());

        let (records, errors) = result.unwrap();
        assert_eq!(records.len(), 3);
        assert_eq!(errors.len(), 0);

        // éªŒè¯Unicodeå†…å®¹è¢«æ­£ç¡®ä¿å­˜
        assert!(records.iter().any(|r| r.description.contains("ä¸­æ–‡å€¼")));
        assert!(records.iter().any(|r| r.description.contains("äº§å“åç§°_ğŸš€")));
        assert!(
            records.iter().any(|r| r.description.contains("completed_å®Œæˆ"))
        );
    }

    #[test]
    fn test_parse_files_concurrent_very_large_files() {
        let temp_dir = TempDir::new().unwrap();

        // åˆ›å»ºå¤§æ–‡ä»¶
        let large_content = create_complex_log_content(500, 0);
        let large_file =
            create_test_log_file(&temp_dir, "large.log", &large_content);

        let parser = ConcurrentParser::new(SqllogConfig {
            thread_count: Some(2),
            batch_size: 100,
            queue_buffer_size: 5000,
        });

        let start_time = std::time::Instant::now();
        let result = parser.parse_files_concurrent(&[large_file]);
        let elapsed = start_time.elapsed();

        assert!(result.is_ok());

        let (records, errors) = result.unwrap();
        assert_eq!(records.len(), 500);
        assert_eq!(errors.len(), 0);

        println!("å¤§æ–‡ä»¶è§£æ: 500æ¡è®°å½•, è€—æ—¶: {:?}", elapsed);
        assert!(elapsed < std::time::Duration::from_secs(5));
    }

    #[test]
    fn test_parse_files_concurrent_many_small_files() {
        let temp_dir = TempDir::new().unwrap();
        let mut files = Vec::new();
        let mut expected_total = 0;

        // åˆ›å»ºå¾ˆå¤šå°æ–‡ä»¶
        for i in 0..20 {
            let record_count = (i % 5) + 1; // 1-5æ¡è®°å½•
            let content = create_complex_log_content(record_count, i * 100);
            let file = create_test_log_file(
                &temp_dir,
                &format!("small_{}.log", i),
                &content,
            );
            files.push(file);
            expected_total += record_count;
        }

        let parser = ConcurrentParser::new(SqllogConfig {
            thread_count: Some(8),
            batch_size: 3,
            queue_buffer_size: 1000,
        });

        let result = parser.parse_files_concurrent(&files);
        assert!(result.is_ok());

        let (records, errors) = result.unwrap();
        assert_eq!(records.len(), expected_total);
        assert_eq!(errors.len(), 0);
    }

    #[test]
    fn test_parse_files_concurrent_thread_count_optimization() {
        let temp_dir = TempDir::new().unwrap();
        let content = create_complex_log_content(50, 0);
        let file = create_test_log_file(&temp_dir, "optimize.log", &content);

        // æµ‹è¯•ä¸åŒçš„çº¿ç¨‹æ•°é…ç½®
        let thread_counts = vec![0, 1, 2, 4, 8];

        for thread_count in thread_counts {
            let parser = ConcurrentParser::new(SqllogConfig {
                thread_count: Some(thread_count),
                batch_size: 10,
                queue_buffer_size: 1000,
            });

            let start_time = std::time::Instant::now();
            let result = parser.parse_files_concurrent(&[file.clone()]);
            let elapsed = start_time.elapsed();

            assert!(result.is_ok(), "Thread count {} failed", thread_count);

            let (records, errors) = result.unwrap();
            assert_eq!(
                records.len(),
                50,
                "Thread count {} wrong record count",
                thread_count
            );
            assert_eq!(
                errors.len(),
                0,
                "Thread count {} has errors",
                thread_count
            );

            println!("çº¿ç¨‹æ•° {}: è€—æ—¶ {:?}", thread_count, elapsed);
        }
    }

    // æµ‹è¯•å¯¼å‡ºåŠŸèƒ½ï¼ˆå¦‚æœå¯ç”¨äº†å¯¼å‡ºfeatureï¼‰
    #[cfg(any(
        feature = "exporter-csv",
        feature = "exporter-json",
        feature = "exporter-sqlite",
        feature = "exporter-duckdb"
    ))]
    mod streaming_export_tests {
        use super::*;
        use std::sync::{Arc, Mutex};
        use std::time::Duration;

        // é«˜çº§æµ‹è¯•å¯¼å‡ºå™¨
        struct AdvancedStreamingExporter {
            name: String,
            exported_records:
                Arc<Mutex<Vec<sqllog_analysis::sqllog::types::Sqllog>>>,
            batch_count: Arc<Mutex<usize>>,
            finalize_called: Arc<Mutex<bool>>,
            delay_per_batch: Option<Duration>,
            fail_on_batch: Option<usize>,
        }

        impl AdvancedStreamingExporter {
            fn new(name: &str) -> Self {
                Self {
                    name: name.to_string(),
                    exported_records: Arc::new(Mutex::new(Vec::new())),
                    batch_count: Arc::new(Mutex::new(0)),
                    finalize_called: Arc::new(Mutex::new(false)),
                    delay_per_batch: None,
                    fail_on_batch: None,
                }
            }

            fn with_delay(mut self, delay: Duration) -> Self {
                self.delay_per_batch = Some(delay);
                self
            }

            fn with_failure_on_batch(mut self, batch_number: usize) -> Self {
                self.fail_on_batch = Some(batch_number);
                self
            }

            #[allow(dead_code)]
            fn get_exported_records(
                &self,
            ) -> Vec<sqllog_analysis::sqllog::types::Sqllog> {
                self.exported_records.lock().unwrap().clone()
            }

            #[allow(dead_code)]
            fn get_batch_count(&self) -> usize {
                *self.batch_count.lock().unwrap()
            }

            #[allow(dead_code)]
            fn is_finalize_called(&self) -> bool {
                *self.finalize_called.lock().unwrap()
            }
        }

        impl SyncExporter for AdvancedStreamingExporter {
            fn name(&self) -> &str {
                &self.name
            }

            fn export_record(
                &mut self,
                record: &sqllog_analysis::sqllog::types::Sqllog,
            ) -> sqllog_analysis::error::Result<()> {
                self.exported_records.lock().unwrap().push(record.clone());
                Ok(())
            }

            fn export_batch(
                &mut self,
                records: &[sqllog_analysis::sqllog::types::Sqllog],
            ) -> sqllog_analysis::error::Result<()> {
                let mut batch_count = self.batch_count.lock().unwrap();
                *batch_count += 1;
                let current_batch = *batch_count;
                drop(batch_count);

                // æ£€æŸ¥å¤±è´¥æ¡ä»¶
                if let Some(fail_batch) = self.fail_on_batch {
                    if current_batch == fail_batch {
                        return Err(
                            sqllog_analysis::error::SqllogError::parse_error(
                                &format!(
                                    "Export failed at batch {}",
                                    current_batch
                                ),
                            )
                            .into(),
                        );
                    }
                }

                // æ·»åŠ å»¶è¿Ÿ
                if let Some(delay) = self.delay_per_batch {
                    std::thread::sleep(delay);
                }

                for record in records {
                    self.exported_records.lock().unwrap().push(record.clone());
                }

                Ok(())
            }

            fn finalize(&mut self) -> sqllog_analysis::error::Result<()> {
                *self.finalize_called.lock().unwrap() = true;
                Ok(())
            }

            fn get_stats(&self) -> ExportStats {
                let mut stats = ExportStats::new();
                stats.exported_records =
                    self.exported_records.lock().unwrap().len();
                stats.finish();
                stats
            }
        }

        #[test]
        fn test_parse_and_export_streaming_basic() {
            let temp_dir = TempDir::new().unwrap();
            let content = create_complex_log_content(10, 0);
            let file =
                create_test_log_file(&temp_dir, "streaming.log", &content);

            let exporter = AdvancedStreamingExporter::new("streaming_basic");
            let exported_records = exporter.exported_records.clone();
            let finalize_called = exporter.finalize_called.clone();

            let parser = ConcurrentParser::new(SqllogConfig {
                thread_count: Some(1),
                batch_size: 3,
                queue_buffer_size: 1000,
            });

            let result = parser.parse_and_export_streaming(&[file], exporter);
            assert!(result.is_ok());

            let results = result.unwrap();
            assert_eq!(results.len(), 1); // 1ä¸ªæ–‡ä»¶
            assert_eq!(results[0].0, 10); // 10æ¡è®°å½•
            assert_eq!(results[0].1, 0); // 0ä¸ªé”™è¯¯

            // éªŒè¯å¯¼å‡º
            let exported = exported_records.lock().unwrap();
            assert_eq!(exported.len(), 10);

            // éªŒè¯finalizeè¢«è°ƒç”¨
            assert!(*finalize_called.lock().unwrap());
        }

        #[test]
        fn test_parse_and_export_streaming_multiple_files() {
            let temp_dir = TempDir::new().unwrap();
            let mut files = Vec::new();
            let mut expected_total = 0;

            // åˆ›å»ºå¤šä¸ªæ–‡ä»¶
            for i in 0..3 {
                let record_count = (i + 1) * 3; // 3, 6, 9
                let content =
                    create_complex_log_content(record_count, i * 1000);
                let file = create_test_log_file(
                    &temp_dir,
                    &format!("multi_{}.log", i),
                    &content,
                );
                files.push(file);
                expected_total += record_count;
            }

            let exporter = AdvancedStreamingExporter::new("streaming_multi");
            let exported_records = exporter.exported_records.clone();
            let batch_count = exporter.batch_count.clone();

            let parser = ConcurrentParser::new(SqllogConfig {
                thread_count: Some(2),
                batch_size: 4,
                queue_buffer_size: 1000,
            });

            let result = parser.parse_and_export_streaming(&files, exporter);
            assert!(result.is_ok());

            let results = result.unwrap();
            assert_eq!(results.len(), 3);

            let total_records: usize = results.iter().map(|(r, _)| r).sum();
            assert_eq!(total_records, expected_total);

            // éªŒè¯å¯¼å‡º
            let exported = exported_records.lock().unwrap();
            assert_eq!(exported.len(), expected_total);

            // éªŒè¯å¤šä¸ªæ‰¹æ¬¡
            assert!(*batch_count.lock().unwrap() > 1);
        }

        #[test]
        fn test_parse_and_export_streaming_with_delays() {
            let temp_dir = TempDir::new().unwrap();
            let content = create_complex_log_content(6, 0);
            let file = create_test_log_file(&temp_dir, "delay.log", &content);

            let start_time = std::time::Instant::now();
            let exporter = AdvancedStreamingExporter::new("streaming_delay")
                .with_delay(Duration::from_millis(50));

            let exported_records = exporter.exported_records.clone();
            let batch_count = exporter.batch_count.clone();

            let parser = ConcurrentParser::new(SqllogConfig {
                thread_count: Some(1),
                batch_size: 2, // 3ä¸ªæ‰¹æ¬¡
                queue_buffer_size: 1000,
            });

            let result = parser.parse_and_export_streaming(&[file], exporter);
            let elapsed = start_time.elapsed();

            assert!(result.is_ok());

            let results = result.unwrap();
            assert_eq!(results[0].0, 6);

            // éªŒè¯æ—¶é—´åŒ…å«å»¶è¿Ÿ
            let expected_batches = *batch_count.lock().unwrap();
            let min_expected_delay =
                Duration::from_millis(50) * expected_batches as u32;
            assert!(elapsed >= min_expected_delay);

            // éªŒè¯è®°å½•
            let exported = exported_records.lock().unwrap();
            assert_eq!(exported.len(), 6);
        }

        #[test]
        fn test_parse_and_export_streaming_export_failure() {
            let temp_dir = TempDir::new().unwrap();
            let content = create_complex_log_content(8, 0);
            let file = create_test_log_file(&temp_dir, "fail.log", &content);

            let exporter = AdvancedStreamingExporter::new("streaming_fail")
                .with_failure_on_batch(2); // ç¬¬äºŒä¸ªæ‰¹æ¬¡å¤±è´¥

            let parser = ConcurrentParser::new(SqllogConfig {
                thread_count: Some(1),
                batch_size: 3,
                queue_buffer_size: 1000,
            });

            let result = parser.parse_and_export_streaming(&[file], exporter);
            // å¯¼å‡ºå¤±è´¥åº”è¯¥è¿”å›é”™è¯¯
            assert!(result.is_err());
        }

        #[test]
        fn test_parse_and_export_streaming_empty_files() {
            let exporter = AdvancedStreamingExporter::new("streaming_empty");

            let parser = ConcurrentParser::new(SqllogConfig::default());

            let result = parser.parse_and_export_streaming(&[], exporter);
            assert!(result.is_ok());

            let results = result.unwrap();
            assert_eq!(results.len(), 0);

            // å¯¹äºç©ºæ–‡ä»¶åˆ—è¡¨ï¼Œä¸å¼ºåˆ¶è¦æ±‚ finalize è¢«è°ƒç”¨ï¼Œå› ä¸ºæ²¡æœ‰å®é™…çš„å¤„ç†å‘ç”Ÿ
        }

        #[test]
        fn test_parse_and_export_streaming_thread_count_zero() {
            let temp_dir = TempDir::new().unwrap();
            let mut files = Vec::new();

            // åˆ›å»º3ä¸ªæ–‡ä»¶
            for i in 0..3 {
                let content = create_complex_log_content(4, i * 100);
                let file = create_test_log_file(
                    &temp_dir,
                    &format!("auto_{}.log", i),
                    &content,
                );
                files.push(file);
            }

            let exporter = AdvancedStreamingExporter::new("streaming_auto");
            let exported_records = exporter.exported_records.clone();

            let parser = ConcurrentParser::new(SqllogConfig {
                thread_count: Some(0), // è‡ªåŠ¨çº¿ç¨‹æ•°
                batch_size: 2,
                queue_buffer_size: 1000,
            });

            let result = parser.parse_and_export_streaming(&files, exporter);
            assert!(result.is_ok());

            let results = result.unwrap();
            assert_eq!(results.len(), 3);

            let total_records: usize = results.iter().map(|(r, _)| r).sum();
            assert_eq!(total_records, 12); // 3 * 4

            let exported = exported_records.lock().unwrap();
            assert_eq!(exported.len(), 12);
        }
    }
}
